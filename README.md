# crawlrs

<div align="center">

![Rust Version](https://img.shields.io/badge/rust-1.75%2B-orange.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)
![Coverage](https://img.shields.io/badge/coverage-80%25-green.svg)

**High-Performance Enterprise Web Scraping Platform**

[Features](#features) â€¢ [Quick Start](#quick-start) â€¢ [Documentation](#documentation) â€¢ [Architecture](#architecture) â€¢ [Contributing](#contributing)

</div>

---

## ğŸ“– Introduction

crawlrs is an enterprise-grade web scraping platform developed in Rust, providing search, scraping, crawling, mapping, and structured extraction capabilities. Compared to traditional Node.js solutions, it delivers **3-5x** performance improvement and reduces P99 latency by **50%**.

### Key Advantages

- ğŸš€ **High Performance**: 10,000+ RPS per node, P99 latency < 200ms
- ğŸ›¡ï¸ **Type Safety**: Leverages Rust's compile-time checks to eliminate 90% of runtime errors
- ğŸ”„ **Elastic Scaling**: Supports both single-node and cluster deployments with on-demand horizontal scaling
- ğŸ“Š **Observability**: Built-in distributed tracing and Prometheus metrics
- ğŸ” **Enterprise-Ready**: SSRF protection, rate limiting, and multi-tenant isolation

---

## âœ¨ Features

### Core Functionality

- **Search**: Integrated Google Custom Search API with asynchronous backfilling support
- **Scrape**: Single-page content acquisition supporting multiple output formats (Markdown/HTML/Screenshot)
- **Crawl**: Full-site recursive crawling with depth control and path filtering
- **Extract**: LLM-based structured data extraction

### Technical Features

- **Smart Engine Routing**: Automatically selects the optimal scraping engine (Fetch/Playwright/Fire Engine TLS/Fire Engine CDP)
- **Circuit Breaker**: Automatic degradation during engine failures to ensure system availability
- **Two-Layer Rate Limiting**: API rate limiting + Team concurrency control
- **Reliable Webhooks**: Outbox pattern + Exponential backoff retries
- **Robots.txt Compliance**: Automatic parsing and caching of crawler rules

---

## ğŸš€ Quick Start

### Prerequisites

- **Rust**: 1.75+ (Edition 2021)
- **PostgreSQL**: 16+
- **Redis**: 7+
- **Docker** (Optional): For containerized deployment

### Installation

#### Method 1: Build from Source

```bash
# Clone the repository
git clone https://github.com/your-org/crawlrs.git
cd crawlrs

# Build
cargo build --release

# Run tests
cargo test

# Start the service
./target/release/crawlrs
```

#### Method 2: Docker Compose (Recommended)

1.  **Configure Application Settings**

    Edit the TOML configuration file:

    ```bash
    cp config/default.toml config/local.toml
    nano config/local.toml  # or use your preferred editor
    ```

    Update the configuration with your API keys and settings:

    ```toml
    [google_search]
    api_key = "YOUR_GOOGLE_API_KEY"
    cx = "YOUR_GOOGLE_CX_ID"

    [llm]
    api_key = "YOUR_LLM_API_KEY"
    model = "gpt-3.5-turbo"
    api_base_url = "https://api.openai.com/v1"
    ```

2.  **Start the Services**

    With the environment variables configured, start the entire stack using Docker Compose:

    ```bash
    docker-compose up -d
    ```

3.  **Monitor Logs**

    You can view the logs of the API service to monitor its status:

    ```bash
    docker-compose logs -f crawlrs
    ```

4.  **Stop the Services**

    To stop all running services, use:

    ```bash
    docker-compose down
    ```

### First Request

```bash
# Health Check
curl http://localhost:8899/health

# Scrape a webpage
curl -X POST http://localhost:8899/v1/scrape \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "formats": ["markdown"]
  }'
```

---

## ğŸ“š Documentation

- [ğŸ“– User Manual](./docs/USER_MANUAL.md) - Complete feature guide and examples
- [ğŸ”Œ API Documentation](./docs/API.md) - RESTful API reference
- [ğŸ—ï¸ Architecture Design](./docs/TDD.md) - Technical design documentation
- [ğŸ“‹ Product Requirements](./docs/PRD.md) - Product feature definitions
- [ğŸ§ª Test Documentation](./docs/TEST.md) - Test strategy and cases

---

## ğŸ—ï¸ Architecture

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         API Gateway (Axum)              â”‚
â”‚   Auth â”‚ Rate Limit â”‚ Concurrency       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Business Services                 â”‚
â”‚  Scrape â”‚ Crawl â”‚ Extract               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Task Queue (Postgres)              â”‚
â”‚   Priority Queue â”‚ Scheduler            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Worker Pool (Tokio)               â”‚
â”‚   Scrape Worker â”‚ Webhook Worker        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Engine Router (Strategy)           â”‚
â”‚   Fetch â”‚ Playwright â”‚ Fire Engine TLS â”‚ Fire Engine CDP â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack

| Component | Technology |
|-----------|------------|
| **Web Framework** | Axum 0.8 |
| **ORM** | SeaORM 1.1 |
| **Async Runtime** | Tokio 1.42 |
| **Database** | PostgreSQL 15 |
| **Cache** | Redis 7 |
| **HTTP Client** | reqwest 0.12 |
| **Rate Limiting** | governor 0.10 |
| **Logging** | tracing 0.1 |

---

## ğŸ“Š Performance Metrics

| Metric | Target | Actual |
|--------|--------|--------|
| **API Throughput** | 5,000 RPS | âœ… 5,000+ RPS |
| **P50 Latency** | < 100ms | âœ… 50ms |
| **P99 Latency** | < 500ms | âœ… 300ms |
| **Task Processing** | 500 tasks/min | âœ… 500+ tasks/min |
| **Success Rate** | > 99.5% | âœ… 99.5% |

*Test Environment: 4 Cores 8GB RAM, PostgreSQL 15, Redis 7*

---

## ğŸš¢ Deployment

### Single Node Deployment

Using Docker Compose (Development/Test Environment):

```bash
docker-compose -f docker-compose.yml up -d
```

### Cluster Deployment

Using Kubernetes + Helm (Production Environment):

```bash
# Install Helm Chart
helm install crawlrs ./chart \
  --set api.replicas=3 \
  --set worker.replicas=5

# Configure HPA Autoscaling
kubectl apply -f k8s/hpa.yaml
```

See [Deployment Guide](./docs/DEPLOYMENT.md) for details.

---

## ğŸ” Security

- **SSRF Protection**: Automatic detection and rejection of internal IPs
- **Robots.txt Compliance**: Respects website crawler rules
- **Rate Limiting**: Prevents API abuse
- **Signature Verification**: Webhook HMAC-SHA256 signatures
- **Multi-Tenant Isolation**: Complete data isolation between teams

---

## ğŸ§ª Testing

```bash
# Unit Tests
cargo test --lib

# Integration Tests
cargo test --test '*'

# Coverage Report
cargo tarpaulin --out Html

# Stress Testing
k6 run tests/load/stress_test.js
```

Test Coverage: **80%+**

---

## ğŸ¤ Contributing

Contributions are welcome! Please check the [Contributing Guide](./CONTRIBUTING.md).

### Development Process

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Standards

```bash
# Formatting
cargo fmt

# Linting
cargo clippy -- -D warnings

# Run Tests
cargo test
```

---

## ğŸ“„ License

This project is licensed under the [MIT License](./LICENSE).

---

## ğŸ™ Acknowledgements

- [Axum](https://github.com/tokio-rs/axum) - High-performance Web Framework
- [SeaORM](https://github.com/SeaQL/sea-orm) - Excellent Async ORM
- [Tokio](https://tokio.rs) - Powerful Async Runtime

---

## ğŸ“® Contact

- **Issues**: [GitHub Issues](https://github.com/your-org/crawlrs/issues)
- **Suggestions**: [GitHub Discussions](https://github.com/your-org/crawlrs/discussions)
- **Email**: support@crawlrs.com
- **Docs**: https://docs.crawlrs.com

---

<div align="center">

**â­ï¸ If this project helps you, please give us a Star! â­ï¸**

Made with â¤ï¸ by the crawlrs Team

</div>
