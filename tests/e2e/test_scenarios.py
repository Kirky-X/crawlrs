#!/usr/bin/env python3
# Copyright (c) 2025 Kirky.X
#
# Licensed under the MIT License
# See LICENSE file in the project root for full license information.

"""
ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶ - å®Œæ•´ä¸šåŠ¡æµç¨‹éªŒè¯

è¯¥æ¨¡å—åŒ…å«å¯¹ crawlrs ç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½çš„ç«¯åˆ°ç«¯æµ‹è¯•ï¼ŒéªŒè¯ä»ä»»åŠ¡åˆ›å»ºåˆ°ç»“æœè·å–çš„å®Œæ•´ä¸šåŠ¡æµç¨‹ã€‚
"""

import requests
import time
import json
import uuid
from typing import Dict, Any, Optional, List
from concurrent.futures import ThreadPoolExecutor, as_completed

# åŸºç¡€é…ç½®
BASE_URL = "http://localhost:3000"
HEADERS = {
    "Authorization": "Bearer test-api-key",
    "Content-Type": "application/json"
}

# æµ‹è¯•æ•°æ®
TEST_URLS = {
    "simple": "https://httpbin.org/html",
    "complex": "https://httpbin.org/json",
    "javascript": "https://httpbin.org/html",  # æ¨¡æ‹Ÿéœ€è¦JSæ¸²æŸ“çš„é¡µé¢
    "error": "https://httpbin.org/status/500"
}

def create_task(endpoint: str, payload: Dict[str, Any]) -> Optional[str]:
    """åˆ›å»ºä»»åŠ¡å¹¶è¿”å›ä»»åŠ¡ID"""
    try:
        response = requests.post(f"{BASE_URL}{endpoint}", json=payload, headers=HEADERS)
        if response.status_code in [201, 202]:
            result = response.json()
            return result.get("id")
        else:
            print(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        print(f"åˆ›å»ºä»»åŠ¡å¼‚å¸¸: {e}")
        return None

def get_task_status(task_id: str, endpoint: str = "/v1/scrape") -> Optional[Dict[str, Any]]:
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        response = requests.get(f"{BASE_URL}{endpoint}/{task_id}", headers=HEADERS)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {response.status_code}")
            return None
    except Exception as e:
        print(f"è·å–ä»»åŠ¡çŠ¶æ€å¼‚å¸¸: {e}")
        return None

def wait_for_task_completion(task_id: str, endpoint: str = "/v1/scrape", 
                           timeout: int = 60, poll_interval: int = 2) -> Optional[Dict[str, Any]]:
    """ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶è¿”å›æœ€ç»ˆç»“æœ"""
    start_time = time.time()
    
    while time.time() - start_time < timeout:
        status = get_task_status(task_id, endpoint)
        if not status:
            return None
            
        task_status = status.get("status", "").lower()
        
        if task_status == "completed":
            return status
        elif task_status == "failed":
            return status
        elif task_status in ["cancelled", "timeout"]:
            return status
        
        time.sleep(poll_interval)
    
    print(f"ä»»åŠ¡ {task_id} è¶…æ—¶æœªå®Œæˆ")
    return None

def cancel_task(task_id: str, endpoint: str = "/v1/scrape") -> bool:
    """å–æ¶ˆä»»åŠ¡"""
    try:
        response = requests.delete(f"{BASE_URL}{endpoint}/{task_id}", headers=HEADERS)
        return response.status_code == 204
    except Exception as e:
        print(f"å–æ¶ˆä»»åŠ¡å¼‚å¸¸: {e}")
        return False

def test_scrape_basic():
    """æµ‹è¯•åŸºç¡€æŠ“å–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€æŠ“å–åŠŸèƒ½...")
    
    payload = {
        "url": TEST_URLS["simple"],
        "task_type": "scrape",
        "payload": {
            "extract_rules": {
                "title": {
                    "selector": "title",
                    "is_array": False
                }
            }
        }
    }
    
    task_id = create_task("/v1/scrape", payload)
    if not task_id:
        print("âŒ åˆ›å»ºæŠ“å–ä»»åŠ¡å¤±è´¥")
        return False
    
    print(f"âœ… åˆ›å»ºä»»åŠ¡æˆåŠŸ: {task_id}")
    
    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
    result = wait_for_task_completion(task_id)
    if not result:
        print("âŒ ä»»åŠ¡æœªå®Œæˆæˆ–å¤±è´¥")
        return False
    
    # éªŒè¯ç»“æœ
    if result.get("status") == "completed":
        print("âœ… ä»»åŠ¡æˆåŠŸå®Œæˆ")
        
        # éªŒè¯æå–çš„å†…å®¹
        content = result.get("content", {})
        if content.get("title"):
            print(f"âœ… æˆåŠŸæå–æ ‡é¢˜: {content['title']}")
            return True
        else:
            print("âŒ æœªæå–åˆ°æ ‡é¢˜å†…å®¹")
            return False
    else:
        print(f"âŒ ä»»åŠ¡çŠ¶æ€: {result.get('status')}")
        print(f"é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return False

def test_crawl_basic():
    """æµ‹è¯•åŸºç¡€çˆ¬å–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€çˆ¬å–åŠŸèƒ½...")
    
    payload = {
        "url": TEST_URLS["simple"],
        "crawler_options": {
            "max_depth": 1,
            "limit": 10,
            "strategy": "bfs"
        }
    }
    
    task_id = create_task("/v1/crawl", payload)
    if not task_id:
        print("âŒ åˆ›å»ºçˆ¬å–ä»»åŠ¡å¤±è´¥")
        return False
    
    print(f"âœ… åˆ›å»ºçˆ¬å–ä»»åŠ¡æˆåŠŸ: {task_id}")
    
    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
    result = wait_for_task_completion(task_id, "/v1/crawl")
    if not result:
        print("âŒ çˆ¬å–ä»»åŠ¡æœªå®Œæˆæˆ–å¤±è´¥")
        return False
    
    # éªŒè¯ç»“æœ
    if result.get("status") == "completed":
        print("âœ… çˆ¬å–ä»»åŠ¡æˆåŠŸå®Œæˆ")
        
        # éªŒè¯çˆ¬å–ç»“æœ
        urls_crawled = result.get("urls_crawled", 0)
        print(f"âœ… çˆ¬å–URLæ•°é‡: {urls_crawled}")
        
        if urls_crawled > 0:
            return True
        else:
            print("âŒ æœªçˆ¬å–åˆ°ä»»ä½•URL")
            return False
    else:
        print(f"âŒ çˆ¬å–ä»»åŠ¡çŠ¶æ€: {result.get('status')}")
        print(f"é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return False

def test_search_basic():
    """æµ‹è¯•åŸºç¡€æœç´¢åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€æœç´¢åŠŸèƒ½...")
    
    payload = {
        "query": "rust programming language",
        "sources": ["web"],
        "limit": 5
    }
    
    task_id = create_task("/v1/search", payload)
    if not task_id:
        print("âŒ åˆ›å»ºæœç´¢ä»»åŠ¡å¤±è´¥")
        return False
    
    print(f"âœ… åˆ›å»ºæœç´¢ä»»åŠ¡æˆåŠŸ: {task_id}")
    
    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
    result = wait_for_task_completion(task_id, "/v1/search")
    if not result:
        print("âŒ æœç´¢ä»»åŠ¡æœªå®Œæˆæˆ–å¤±è´¥")
        return False
    
    # éªŒè¯ç»“æœ
    if result.get("status") == "completed":
        print("âœ… æœç´¢ä»»åŠ¡æˆåŠŸå®Œæˆ")
        
        # éªŒè¯æœç´¢ç»“æœ
        data = result.get("data", {})
        web_results = data.get("web", [])
        
        if web_results and len(web_results) > 0:
            print(f"âœ… æœç´¢åˆ° {len(web_results)} ä¸ªç»“æœ")
            # éªŒè¯ç¬¬ä¸€ä¸ªç»“æœçš„ç»“æ„
            first_result = web_results[0]
            if all(key in first_result for key in ["title", "url", "snippet"]):
                print("âœ… æœç´¢ç»“æœæ ¼å¼æ­£ç¡®")
                return True
            else:
                print("âŒ æœç´¢ç»“æœæ ¼å¼ä¸æ­£ç¡®")
                return False
        else:
            print("âŒ æœªæœç´¢åˆ°ä»»ä½•ç»“æœ")
            return False
    else:
        print(f"âŒ æœç´¢ä»»åŠ¡çŠ¶æ€: {result.get('status')}")
        print(f"é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return False

def test_extract_basic():
    """æµ‹è¯•åŸºç¡€æå–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€æå–åŠŸèƒ½...")
    
    payload = {
        "urls": [TEST_URLS["simple"]],
        "prompt": "Extract the page title and any headings (h1, h2, h3)"
    }
    
    task_id = create_task("/v1/extract", payload)
    if not task_id:
        print("âŒ åˆ›å»ºæå–ä»»åŠ¡å¤±è´¥")
        return False
    
    print(f"âœ… åˆ›å»ºæå–ä»»åŠ¡æˆåŠŸ: {task_id}")
    
    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
    result = wait_for_task_completion(task_id, "/v1/extract")
    if not result:
        print("âŒ æå–ä»»åŠ¡æœªå®Œæˆæˆ–å¤±è´¥")
        return False
    
    # éªŒè¯ç»“æœ
    if result.get("status") == "completed":
        print("âœ… æå–ä»»åŠ¡æˆåŠŸå®Œæˆ")
        
        # éªŒè¯æå–ç»“æœ
        data = result.get("data", {})
        if data:
            print(f"âœ… æˆåŠŸæå–æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
            return True
        else:
            print("âŒ æœªæå–åˆ°ä»»ä½•æ•°æ®")
            return False
    else:
        print(f"âŒ æå–ä»»åŠ¡çŠ¶æ€: {result.get('status')}")
        print(f"é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return False

def test_task_cancellation():
    """æµ‹è¯•ä»»åŠ¡å–æ¶ˆåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ä»»åŠ¡å–æ¶ˆåŠŸèƒ½...")
    
    # åˆ›å»ºä¸€ä¸ªé•¿æ—¶é—´è¿è¡Œçš„çˆ¬å–ä»»åŠ¡
    payload = {
        "url": "https://httpbin.org/html",
        "crawler_options": {
            "max_depth": 3,
            "limit": 100,
            "strategy": "bfs"
        }
    }
    
    task_id = create_task("/v1/crawl", payload)
    if not task_id:
        print("âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥")
        return False
    
    print(f"âœ… åˆ›å»ºä»»åŠ¡æˆåŠŸ: {task_id}")
    
    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©ä»»åŠ¡å¼€å§‹
    time.sleep(3)
    
    # å°è¯•å–æ¶ˆä»»åŠ¡
    if cancel_task(task_id, "/v1/crawl"):
        print("âœ… ä»»åŠ¡å–æ¶ˆè¯·æ±‚æˆåŠŸ")
        
        # ç­‰å¾…å¹¶éªŒè¯ä»»åŠ¡çŠ¶æ€
        time.sleep(2)
        final_status = get_task_status(task_id, "/v1/crawl")
        
        if final_status and final_status.get("status") in ["cancelled", "cancelling"]:
            print("âœ… ä»»åŠ¡å·²æˆåŠŸå–æ¶ˆ")
            return True
        else:
            print(f"âŒ ä»»åŠ¡çŠ¶æ€æœªæ­£ç¡®æ›´æ–°: {final_status.get('status') if final_status else 'æœªçŸ¥'}")
            return False
    else:
        print("âŒ ä»»åŠ¡å–æ¶ˆå¤±è´¥")
        return False

def test_concurrent_tasks():
    """æµ‹è¯•å¹¶å‘ä»»åŠ¡å¤„ç†"""
    print("ğŸ§ª æµ‹è¯•å¹¶å‘ä»»åŠ¡å¤„ç†...")
    
    def create_and_wait_task(task_num: int) -> Dict[str, Any]:
        """åˆ›å»ºå¹¶ç­‰å¾…å•ä¸ªä»»åŠ¡å®Œæˆ"""
        payload = {
            "url": TEST_URLS["simple"],
            "task_type": "scrape",
            "payload": {
                "extract_rules": {
                    "title": {
                        "selector": "title",
                        "is_array": False
                    }
                }
            }
        }
        
        task_id = create_task("/v1/scrape", payload)
        if not task_id:
            return {"success": False, "error": "åˆ›å»ºä»»åŠ¡å¤±è´¥"}
        
        result = wait_for_task_completion(task_id)
        if not result:
            return {"success": False, "error": "ä»»åŠ¡æœªå®Œæˆ"}
        
        return {
            "success": result.get("status") == "completed",
            "task_id": task_id,
            "task_num": task_num
        }
    
    # å¹¶å‘åˆ›å»º5ä¸ªä»»åŠ¡
    num_tasks = 5
    results = []
    
    with ThreadPoolExecutor(max_workers=num_tasks) as executor:
        futures = [executor.submit(create_and_wait_task, i) for i in range(num_tasks)]
        
        for future in as_completed(futures):
            results.append(future.result())
    
    # éªŒè¯ç»“æœ
    successful_tasks = sum(1 for r in results if r.get("success", False))
    
    print(f"âœ… å¹¶å‘ä»»åŠ¡æµ‹è¯•ç»“æœ: {successful_tasks}/{num_tasks} æˆåŠŸ")
    
    if successful_tasks == num_tasks:
        print("âœ… æ‰€æœ‰å¹¶å‘ä»»åŠ¡éƒ½æˆåŠŸå®Œæˆ")
        return True
    else:
        print("âŒ éƒ¨åˆ†å¹¶å‘ä»»åŠ¡å¤±è´¥")
        for result in results:
            if not result.get("success", False):
                print(f"  ä»»åŠ¡ {result.get('task_num')}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return False

def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†æœºåˆ¶"""
    print("ğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†æœºåˆ¶...")
    
    test_cases = [
        {
            "name": "æ— æ•ˆURLæ ¼å¼",
            "payload": {
                "url": "not-a-valid-url",
                "task_type": "scrape",
                "payload": {}
            },
            "expected_status": 422
        },
        {
            "name": "ç¼ºå°‘å¿…éœ€å‚æ•°",
            "payload": {
                "task_type": "scrape",
                "payload": {}
            },
            "expected_status": 422
        },
        {
            "name": "æ— æ•ˆè®¤è¯",
            "payload": {
                "url": "https://example.com",
                "task_type": "scrape",
                "payload": {}
            },
            "headers": {"Authorization": "Bearer invalid-key"},
            "expected_status": 401
        }
    ]
    
    all_passed = True
    
    for test_case in test_cases:
        print(f"  æµ‹è¯•: {test_case['name']}")
        
        headers = test_case.get("headers", HEADERS)
        response = requests.post(f"{BASE_URL}/v1/scrape", 
                               json=test_case["payload"], 
                               headers=headers)
        
        if response.status_code == test_case["expected_status"]:
            print(f"    âœ… è¿”å›æ­£ç¡®çš„çŠ¶æ€ç : {response.status_code}")
        else:
            print(f"    âŒ æœŸæœ›çŠ¶æ€ç  {test_case['expected_status']}, å®é™…: {response.status_code}")
            all_passed = False
    
    return all_passed

def test_rate_limiting():
    """æµ‹è¯•é€Ÿç‡é™åˆ¶æœºåˆ¶"""
    print("ğŸ§ª æµ‹è¯•é€Ÿç‡é™åˆ¶æœºåˆ¶...")
    
    # å¿«é€Ÿå‘é€å¤šä¸ªè¯·æ±‚ä»¥è§¦å‘é€Ÿç‡é™åˆ¶
    payload = {
        "url": "https://example.com",
        "task_type": "scrape",
        "payload": {}
    }
    
    # å‘é€è¶…è¿‡é€Ÿç‡é™åˆ¶çš„è¯·æ±‚ï¼ˆå‡è®¾é™åˆ¶ä¸º100 RPMï¼‰
    rate_limited = False
    
    for i in range(105):
        response = requests.post(f"{BASE_URL}/v1/scrape", json=payload, headers=HEADERS)
        
        if response.status_code == 429:  # Too Many Requests
            rate_limited = True
            print(f"âœ… åœ¨ç¬¬ {i+1} ä¸ªè¯·æ±‚æ—¶è§¦å‘é€Ÿç‡é™åˆ¶")
            break
    
    if rate_limited:
        # éªŒè¯é”™è¯¯å“åº”æ ¼å¼
        error_data = response.json()
        if "error" in error_data and "rate limit" in error_data["error"].lower():
            print("âœ… é€Ÿç‡é™åˆ¶é”™è¯¯ä¿¡æ¯æ­£ç¡®")
            return True
        else:
            print("âŒ é€Ÿç‡é™åˆ¶é”™è¯¯ä¿¡æ¯æ ¼å¼ä¸æ­£ç¡®")
            return False
    else:
        print("âŒ æœªè§¦å‘é€Ÿç‡é™åˆ¶")
        return False

def test_scrape_screenshot():
    """æµ‹è¯•é¡µé¢æˆªå›¾åŠŸèƒ½ (UAT-005)"""
    print("ğŸ§ª æµ‹è¯•é¡µé¢æˆªå›¾åŠŸèƒ½...")
    payload = {
        "url": TEST_URLS["simple"],
        "formats": ["screenshot"]
    }
    
    task_id = create_task("/v1/scrape", payload)
    if not task_id:
        print("âŒ åˆ›å»ºæˆªå›¾ä»»åŠ¡å¤±è´¥")
        return False
        
    result = wait_for_task_completion(task_id)
    if result and result.get("status") == "completed":
        # Check if screenshot data exists
        data = result.get("data", {})
        if "screenshot" in data and data["screenshot"]:
            print(f"âœ… æˆªå›¾ä»»åŠ¡å®Œæˆä¸”åŒ…å«æˆªå›¾æ•°æ®")
            return True
        else:
             print(f"âŒ æˆªå›¾ä»»åŠ¡å®Œæˆä½†ç¼ºå°‘æˆªå›¾æ•°æ®: {data.keys()}")
             return False
    else:
        print(f"âŒ æˆªå›¾ä»»åŠ¡å¤±è´¥: {result.get('status') if result else 'None'}")
        return False

def test_crawl_full():
    """æµ‹è¯•å…¨ç«™çˆ¬å–åŠŸèƒ½ (UAT-006)"""
    print("ğŸ§ª æµ‹è¯•å…¨ç«™çˆ¬å–åŠŸèƒ½...")
    # Use a small real site for full crawl test
    payload = {
        "url": "https://httpbin.org/links/5/0", # Returns a page with 5 links
        "crawler_options": {
            "max_depth": 1,
            "limit": 10
        }
    }
    crawl_id = create_task("/v1/crawl", payload)
    if not crawl_id:
        print("âŒ åˆ›å»ºå…¨ç«™çˆ¬å–ä»»åŠ¡å¤±è´¥")
        return False
    
    # Wait longer for crawl
    result = wait_for_task_completion(crawl_id, endpoint="/v1/crawl", timeout=120)
    if result and result.get("status") == "completed":
        print(f"âœ… å…¨ç«™çˆ¬å–ä»»åŠ¡å®Œæˆ: {crawl_id}")
        # Verify stats if available
        stats = result.get("stats", {})
        print(f"   çˆ¬å–ç»Ÿè®¡: {stats}")
        return True
    else:
        print(f"âŒ å…¨ç«™çˆ¬å–ä»»åŠ¡å¤±è´¥: {result.get('status') if result else 'None'}")
        return False

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶\n")
    
    tests = [
        ("åŸºç¡€æŠ“å–åŠŸèƒ½", test_scrape_basic),
        ("åŸºç¡€çˆ¬å–åŠŸèƒ½", test_crawl_basic),
        ("åŸºç¡€æœç´¢åŠŸèƒ½", test_search_basic),
        ("åŸºç¡€æå–åŠŸèƒ½", test_extract_basic),
        ("é¡µé¢æˆªå›¾åŠŸèƒ½", test_scrape_screenshot),
        ("å…¨ç«™çˆ¬å–åŠŸèƒ½", test_crawl_full),
        ("ä»»åŠ¡å–æ¶ˆåŠŸèƒ½", test_task_cancellation),
        ("å¹¶å‘ä»»åŠ¡å¤„ç†", test_concurrent_tasks),
        ("é”™è¯¯å¤„ç†æœºåˆ¶", test_error_handling),
        ("é€Ÿç‡é™åˆ¶æœºåˆ¶", test_rate_limiting),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            print(f"\n{'='*50}")
            result = test_func()
            results.append((test_name, result))
            
            if result:
                print(f"\nâœ… {test_name} - é€šè¿‡")
            else:
                print(f"\nâŒ {test_name} - å¤±è´¥")
                
        except Exception as e:
            print(f"\nâŒ {test_name} - å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ€»ç»“æŠ¥å‘Š
    print(f"\n{'='*60}")
    print("ğŸ“Š ç«¯åˆ°ç«¯æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print(f"{'='*60}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    print(f"æ€»æµ‹è¯•æ•°: {total}")
    print(f"é€šè¿‡æ•°: {passed}")
    print(f"å¤±è´¥æ•°: {total - passed}")
    print(f"é€šè¿‡ç‡: {(passed/total)*100:.1f}%")
    
    print("\nè¯¦ç»†ç»“æœ:")
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {status} {test_name}")
    
    return passed == total

if __name__ == "__main__":
    success = run_all_tests()
    exit(0 if success else 1)